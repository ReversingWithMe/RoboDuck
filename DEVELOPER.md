# Developer Story

## Flow of control
- `main.py:1-13` instantiates `CRS`, tweaks the garbage collector, and starts `crs.loop()`, so every run is just "ask CRS to keep pulling jobs until the deadline".
  - `CRS` in `crs/app/app.py:85-274` drives the work queue: a `WorkType` enum models every async ingredient (launching fuzzers, running GAN-style analyzers, scoring, triaging, bundling, and submitting) and `CRSWorkDB` registers callbacks for each event.
- Each submitted task lives in `crs/task_server/db.py:22-202` and is emitted via the FastAPI endpoints in `crs/task_server/app.py:1-106`; the `TaskDetail` schema in `crs/task_server/models.py:75-137` explains the deadline, focus path, and zipped sources the parser expects.
- Background workers (`launch_bgworkers()`/`launch_fuzzers()`) pool `crs/modules/fuzzing.FuzzManager` instances, feed coverage/crash jobs back through `CRSWorkDB`, and enqueue `TRIAGE_POV`, `PROCESS_COVERAGE`, and frontier flips that keep the fuzzers productive (`crs/app/app.py:336-445` and `crs/modules/fuzzing.py:1-210`).
- Static analysis jobs (Infer, SARIF, differential analysis, multi-model LLMs) are queued through the same workdb so they can race with fuzzing while sharing the same quantiles/caches (`crs/app/app.py:445-700`).
- Once a vulnerability is confirmed, `handle_analyzed_vuln()` dedupes it via `triage.dedupe_vulns`, writes to `ProductsDB`, and triggers POV/patch bundling plus a `Submitter` call that talks to whatever `CAPI_*` endpoint you configured (`crs/app/app.py:700-950`, `crs/app/submitter.py:19-259`).

## How to feed input
- Tasks and SARIF broadcasts arrive on the FastAPI task server (`crs/task_server/app.py:1-106`). They can be pushed manually with `curl`, via the included sample schedules in `tests/app/schedules/*.yaml`, or replayed with `round_sim.py:1-128`, which reads schedule YAML + JSON, waits until each broadcast time, and posts to `/v1/task/` or `/v1/sarif/`.
- A `TaskDetail` references repositories as a list of `SourceDetail` entries (`crs/task_server/models.py:75-105`); each entry is a gzipped tarball plus SHA256 so the CRS can snapshot the source inside `/data` and reuse it per-task.
- Example task payloads live under `tests/app/tasks/*`; you can replay them against `round_sim` or post them directly to `http://localhost:1324/v1/task/` to exercise the pipeline without needing DARPA services.

## Source code handling
- `ProjectInfo` (`crs/modules/project.py:61-118`) is driven by `project.yaml` inside each repo. It tells CRS what language, sanitizers, architectures, and fuzzing engines are supported.
- `_init_project_data()` (`crs/modules/project.py:168-199`) builds the repo image, caches a `src.tar` per-project, and exposes a `Project` object that bundles `BuildArtifacts`, `SearchCode`, and `EditableOverlayFS` helpers.
- `BuildArtifacts.run()` (`crs/modules/project.py:220-242`) prepends build-time env vars, mounts `/out`, and allows CRS to execute harnesses, reproduce POVs, and write coverage data through the shared `RUNNER_IMAGE`. That is the surface that `FuzzManager`, `CoverageAnalyzer`, `PovProducer`, and patch producers all share.

## AI usage and analysis story
- `crs/config.py:100-147` seeds `litellm`, sets `MODEL_MAP`, and loads API keys from `tokens_etc`, so every agent call can switch to the per-task model config even when pipelines run on Claude, Anthropic, Gemini, or OpenAI.
- `analysis/integration.get_ainalysis_reports()` (`crs/analysis/integration.py:1-48`) glues the `analysis/full.py` LLM pass into a `VulnReport`, logs the LLM queries, and hands the results back to `CRS` so they can be scored and triaged at the same priority as Infer/SARIF.
- `CRSVuln` in `crs/agents/vuln_analyzer.py:1-158` runs two stages: a classification pass (`LikelyVulnClassifier.batch_classify`) to gate-report probability and a multi-tool `XMLAgent` session that produces binary answers (`triggerable`, notes, and an `AnalyzedVuln`). These agents call `SearchCode` tools so the LLM learns the shape of the repo before recommending POV/patch work.
- Encoder/decoder generation (`crs/app/app.py:360-460`) also wraps AI tools: `generate_kaitai`, `harness_input_decoder`, and `pov_producer` are all Litellm-powered agents that turn seeds into machine-understandable input/decoders, then stash them in `ProductsDB` for reuse.
- All logger spans include agent/task metadata (`crs/config.py:147-193`) so you can trace every prompt/completion inside the observability stack (OTel metrics, json logs, and `agent-log-viewer`).

- Stage 0/1 (task ingestion + fuzzing) don’t touch these models; they focus on FastAPI ingestion (crs/task_server/app.py:1-106) and fuzzing workers (crs/app/app.py:336-399). So ignore the list for those stages.
- Stage 2 (LLM/static analysis):  
  * FullMode (crs/analysis/full.py:188-260) and FullModeMulti (crs/analysis/full.py:265-390) are the single- and multi-function prompts described in prompts/default.yaml:1186-1545. FullMode gets your per-function “actions/sinks/vulns/invariants” logic, while FullModeMulti handles chunk-level “summary/allocations/vulns”. The model lists you cite (configs/models-final.toml:17-19) determine which LLMs get invoked in each loop inside launch_ainalysis (crs/app/app.py:561-623). Every parsed SourceMember is eligible for single-mode unless filter_members in analysis/full.py:231-260 drops it; multi-mode builds chunks via the token frequency logic in analyze_project_multifunc and feeds those to the same shared tree-sitter data.  
  * LikelyVulnClassifier (prompts/default.yaml:933-954, crs/agents/vuln_analyzer.py:88-108) is used right after stage 2 produces each VulnReport; score_vuln (crs/app/app.py:323-354) calls it to generate the numeric likelihood that gates stage 3.  
  * CRSVulnAnalyzerAgent (crs/agents/vuln_analyzer.py:60-158) is the expensive follow-up run in analyze_vuln (crs/app/app.py:995-1045) that produces AnalyzedVulns if the classifier score passes the thresholds.  
  * DedupClassifier, FunctionSummarizer, SourceQuestionsAgent, AddDebugPrintsAgent, etc., are utility agents referenced throughout the CRS workflow (e.g., triage, pov production, debug helpers), but not part of the core stage 2 LLM loops—they run later during POV/patch generation or triage (crs/app/submitter.py, crs/agents/dedup.py, crs/agents/pov_producer.py, etc.).
- Stage 3 (scoring/triage and submission):  
  * LikelyVulnClassifier scores each VulnReport, and high-scoring reports go to analyze_vuln where CRSVulnAnalyzerAgent performs the agent-driven validation.  
  * CRSDiffAgent is used solely inside analyze_diff (crs/app/app.py:964-984) to inspect git diffs and submit VulnReports directly into the triage pipeline without the classifier gate.  
  * TriageAgent/DedupClassifier/BranchFlipperAgent/CRSPovProducerAgent/HarnessInputEncoderAgent/HarnessInputDecoderAgent/GenerateKaitaiAgent/PatcherAgent orchestrate the downstream work once a vuln is accepted (handle_analyzed_vuln and Submitter in crs/app/submitter.py:19-259), handling dedupe, POV creation, block analysis, and patch proposals; they are not used during stage 2 itself but kick in after a vuln passes stage 3 gating.  
  * LikelyFlippableClassifier helps determine whether a control-flow branch is worth fuzzing; BranchFlipperAgent actually flips it—these belong to the fuzzing/POV stage rather than stage 2.
So, in summary: FullMode/FullModeMulti determine which prompts run (single vs. multi), LikelyVulnClassifier scores their reports, and CRSVulnAnalyzerAgent performs the expensive validation. CRSDiffAgent is independent (stage 3 diff analysis), and the other agents manage triage/POV/patch once a vuln survives the scoring path.

## Submissions and scoring
- `Submitter` (`crs/app/submitter.py:19-259`) wraps `CompetitionAPIClient`, polls for acceptance, and stores every POV/patch/bundle sent into `ProductsDB`. It uses `config.CAPI_URL/CAPI_ID/CAPI_TOKEN` by default (`crs/config.py:100-133`), so replacing the client is the key to running without DARPA.

## Data
- `ProductsDB` (`crs/app/products_db.py:48-886`) is the state plane for every artifact. Its schema covers `reports`, `vulns`, `povs`, `patches`, `bundles`, `submissions`, `decoders`, and `encoders`, and each `add_*`/`get_*` helper (e.g., `add_reports` at `crs/app/products_db.py:187-210`, `add_povs` at `:237-261`, `get_vulns_for_task` at `:342-356`) keeps the cross-references intact. The file path is `config.DATA_DIR / "products.sqlite3"` (`ProductsDB.__init__` at `crs/app/products_db.py:171-175`).
- The work queue persists to `config.DATA_DIR / "work.sqlite3"` through `WorkDB` (`crs/common/workdb.py:33-201`). The `jobs` table schema (`crs/common/workdb.py:33-46`) stores each `worktype`, serialized `task_desc`, `priority`, and `expiration`, and `CRSWorkDB` (`crs/app/app.py:85-331`) wires callbacks so `WorkType` enum members can enqueue and rerun failed work.
- Task ingestion happens in `TaskDB` (`crs/task_server/db.py:22-202`). Its tables track `tasks`, `sarifs`, `statuses`, and `cancellations`, with helpers such as `put_tasks`/`get_tasks`/`cancel_task` so `crs/task_server/app.py:1-106` can snapshot JSON payloads, deque cancellations, and surface `/v1/status/`.
- Counters are sharded per-task in `CounterDB` (`crs/app/counter_db.py:1-104`). The single `counters` table stores (task, name, value) tuples under `config.DATA_DIR / "counters.sqlite3"`, and `view()` (`crs/app/counter_db.py:79-99`) exposes async gauges that feed `QuantileEstimator` and the spend limiter used during `analyze_vuln`.

## Analysis stages

- **Stage relations**: Stage 0 feeds Stage 1/2 by dismantling tasks into `CRSWorkDB` jobs; fuzzing in Stage 1 feeds coverage/crash artifacts and POV candidates back into `ProductsDB`/`WorkDB` so static analyzers (Stage 2) and diff/LLM scoring (Stage 3) have data, while Stage 2’s `VulnReport`s and Stage 3 scoring results trigger `handle_analyzed_vuln`, which in turn enqueues patch/POV bundling and submissions (Stage 4). When a model rejects a bug, the same work queue retries without losing DB context, keeping the stages tightly coupled.
    - **Core libraries/services**: Stage 0-3 lean on several shared dependencies:
      - `tree-sitter` with the bundled C/Java grammars (`crs/analysis/c_tree_sitter.py`, `crs/analysis/java_tree_sitter.py`) fills `AnalysisProject` via `load_vfs` (`crs/analysis/vfs_loader.py:10-41`), letting LLM prompts reference the AST-backed `SourceMember`s.
      - `litellm` orchestrates every AI call,
      - `maturin`/`cargo` compile the `crs_rust` extension
      - and `aiosqlite`/`fastapi` back the task/work/product databases.
        - `uvicorn` serves the task server at `/v1/task`, and `requests/typed dicts` hold the metadata exchanged between stages.

- **Stage 0 (task ingestion + scaffolding)**: `crs/task_server/app.py:1-106` accepts `/v1/task/` and `/v1/sarif/` posts, persists them via `TaskDB` (`crs/task_server/db.py:22-202`), and `CRS` materializes each `TaskDetail` before scheduling `WorkType.LAUNCH_*` jobs (`crs/app/app.py:85-274`).
- **Stage 1 (fuzzing + coverage/crash handling)**: `launch_bgworkers` (`crs/app/app.py:336-358`) spins up `FuzzManager`, routes coverage seeds (`PROCESS_COVERAGE` jobs) and crashes (`TRIAGE_FUZZ_CRASH`) to `BulkCoverageWorker/BulkCrashWorker` (`crs/modules/fuzzing.py:1-200`), and drives triage/pov submissions. `launch_fuzzers` (`crs/app/app.py:361-399`) keeps libFuzzer running until the deadline.

- **Stage 2 (static & AI analysis)**: `launch_infer`/`launch_ainalysis`/`launch_sarif` in `crs/app/app.py:561-717`, so refer there first: `CRS.loop()` schedules these workers in parallel once the task’s `WorkType.LAUNCH_*` jobs fire (`crs/app/app.py:85-274`). AI analyzes chunks of functions, infer runs the meta infer program to find issues, and sarif ingests sarif files from other analyses (SARIF reports) to populate vulenrability reports in a consistent style. Treesitter is used to drive the AI analysis. The “tree-sitter boot” happens before any LLM/Infer work—`load_vfs` (`crs/analysis/vfs_loader.py:10-41`) which drives the parser (`crs/analysis/c_tree_sitter.py` and `crs/analysis/java_tree_sitter.py`) over each .c/.java file to build `SourceFile`/`SourceMember` objects, then `analysis/full.py:188-390` (single-function) and `analysis/full.py:265-460` (multi-function) pull from that `AnalysisProject`. `launch_ainalysis` iterates the configured models in `configs/models-final.toml:17-19`, running the `FullMode` list for single-function prompts and the `FullModeMulti` list for chunked prompts; so the “decision” about single vs. multi is purely “which model list contains entries,” and both sets draw from the same tree-sitter data. Single prompts are invoked for every `SourceMember` that survives the `filter_members` pass in `analysis/full.py:247-260`, so each parsed/filtered function becomes a single-function query unless `filter_members` prunes it (e.g., too small or contains unsupported language). Multi prompts are fed by analyze_project_multifunc (`crs/analysis/full.py:265-390`): after all `SourceFiles` are collected, a token-frequency heuristic clusters files into language-group chunks (`LanguageGroup` in lines 258-345), and the code keeps adding files until the chunk size nears the splitsize threshold (roughly twice the model’s max token budget) before sending the chunk to the prompt. Each AI call returns a fenced YAML result, parsed by `analysis/parser.py:16-198` into Report/MultiReport structs, while `analysis/integration.get_ainalysis_reports` (`crs/analysis/integration.py:1-48`) logs the prompt/response pair, maps each parsed entry into a `VulnReport` (with path, function, source, reason), and requeues it as `ANALYZE_VULN` work for the queue. `launch_infer` runs Infer + StaticAnalyzer in parallel and publishes its own VulnReports (via the same ANALYZE_VULN job creation path in `crs/app/app.py:561-598`), and `launch_sarif` ingests SARIF payloads at `/v1/sarif` to shape additional VulnReports (lines 624-717). Because all three sources produce the same VulnReport schema, the downstream scoring/triage (`score_vuln/analyze_vuln` in `crs/app/app.py:724-1108`) doesn’t care about their origin; it just processes the queue. Each piece—tree-sitter parsing, single/multi LLM runs, Infer, and SARIF ingestion—happens curb-parallel inside their own worker loops, but they converge when the `ANALYZE_VULN` work is enqueued and the VulnReport records are generated, preserving a unified view of stage 2’s output. The multi function analysis appears to be the most complex to understand at a glance.
- **Stage 3 (diff/LLM scoring & triage)**: (NOTE this requires gpt model for logprobs) Stage 3 consumes the `ANALYZE_VULN` work items that stage 2 enqueues once either the immediate `launch_ainalysis`/`launch_infer` copies insert reports (the `WorkType.ANALYZE_VULN` submission is done in `crs/app/app.py:701-716` and `:732-754` for SARIF) or the scoring loop decides a report is worth deeper scrutiny. The first worker is score_vuln (`crs/app/app.py:323-354`): it loads the VulnReport from `ProductsDB`, calls `CRSVuln.score_vuln_report` to get a numerical likelihood, and only when the report beats both the task quantile and the `GLOBAL_SCORE_THRESHOLD` does it requeue another `ANALYZE_VULN` job with priority proportional to how confident the model was; this gate keeps the queue from being overwhelmed. The actual `ANALYZE_VULN handler` (`crs/app/app.py:995-1045`) acquires a spend-limited safety gate, invokes `CRSVuln.analyze_vuln_report` with a randomized model_idx (module `crs/agents/vuln_analyzer.py`), and if the LLM approves it stores the resulting AnalyzedVuln, dedupes it via `triage.dedupe_vulns`, and may trigger POV/patch work (see handle_analyzed_vuln immediately afterward). When git-diff or patches are provided, `analyze_diff` (`crs/app/app.py:964-988`) is the Stage 3 path that runs independently of the LLM flow: it calls `diff_analyzer.CRSDiff.from_task(task).analyze_diff()` twice (with and without pruning) and each returned VulnReport goes straight into `handle_analyzed_vuln` (`crs/app/app.py:1117-1149`), bypassing the `score_vuln` LLM gate entirely. `diff_analyzer` inspects the git delta rooted in the task, runs custom heuristics (no Litellm prompt involved), and emits structured vulnerability descriptions about the changed lines; once emitted, those reports enter the same `VulnReport` stream as the AI/Infer results for deduplication/POV submission. Each analyze_vuln run spins up `CRSVulnAnalyzerAgent` (`crs/agents/vuln_analyzer.py:111-158`), which produces an `AnalyzedVuln` and hands it to `handle_analyzed_vuln` (`crs/app/app.py:1102-1208`). Inside `handle_analyzed_vuln`, `triage.dedupe_vulns` (`crs/agents/triage.py:72-106`, using the `DedupClassifier` prompt from prompts/default.yaml) compares the new root cause against prior vulns, and if it’s new the `TriageAgent` (`crs/agents/triage.py:14-69`) is launched (tools: read definition/source, find references, debug POV). Once deduped/triaged, the submitted vuln flows through `Submitter` (crs`/app/submitter.py:19-259`), which calls `CRSPovProducerAgent`, `HarnessInputEncoderAgent`, `HarnessInputDecoderAgent`, `GenerateKaitaiAgent`, and `PatcherAgent` (all wired via configs/models-final.toml) to create POVs/patches before the DARPA client would send them. Diff-driven reports (`CRSDiffAgent` from `crs/app/app.py:964-988`) skip the scoring gate but reuse handle_analyzed_vuln, so every successful report eventually surfaces through the same triage → POV agent sequence.
  - The numeric likelihood in score_vuln is the “likely/unlikely” probability that `CRSVuln.score_vuln_report` (in `crs/agents/vuln_analyzer.py:88-108`) derives by calling `LikelyVulnClassifier.batch_classify`. That method feeds the vulnerability description + nearby source into the AI classifier, which uses the prompt block you can inspect at `prompts/default.yaml:933-954` (search for LikelyVulnClassifier). The prompt tells the model to weigh whether the reported vuln is triggerable via user input and to explain its reasoning, so the classifier’s `ClassifierBatchResult` holds a distribution over likely vs unlikely; the `.overall()` helper (lines 80‑86) returns `result.max("likely")`, and that floating‑point value is what score_vuln (`crs/app/app.py:323-354`) compares against the quantiles and the `GLOBAL_SCORE_THRESHOLD`. If the score passes both thresholds, the report is scheduled for the more expensive `analyze_vuln` path; otherwise it stays filtered. `analyze_vuln` (`crs/app/app.py:995-1045`), score_vuln believes a `VulnReport` is rare enough (above the quantile and `GLOBAL_SCORE_THRESHOLD`), it resubmits `WorkType.ANALYZE_VULN` at higher priority so `analyze_vuln` can run. That worker grabs a spend limiter (`_get_spend_limiter`), calls `CRSVuln.analyze_vuln_report` (see `crs/agents/vuln_analyzer.py:111-158`) with a randomly selected model index, and the agent (an LLM session via `CRSVulnAnalyzerAgent`) runs up to 30 tool-enabled iterations before either producing a VulnAnalysis or being forced to terminate. If the agent flags the report as triggerable, `analyze_vuln` hands the resulting `AnalyzedVuln` to handle_analyzed_vuln for dedupe/POV/patch workIn short, the prompt at `prompts/default.yaml:933-954` is what produces the LLM logits, and `CRSVuln.score_vuln_report` + `score_vuln` turn that into the numeric likelihood that gates stage 3’s deeper analysis.

- **Stage 4 (patch/POV bundling + submission)**: `handle_analyzed_vuln` onwards (`crs/app/app.py:1102-1208`) uses `ProductsDB` to store vulns/patches/POVs, builds bundles, and hands them to `Submitter`/`CompetitionAPIClient` (`crs/app/submitter.py:19-259`) so DARPA scoring sees the artifact. Bundles also route back through `ProductsDB` so `round_sim`/dashboard tooling can visualize them.

## Irrelevant directories (for faster triage)
- `external/infer`, `external/bear`, `external/llvm-cov`: blobs needed only to build the Infer/Bear/Linux coverage toolchain. You can ignore them unless you are tuning static-analysis builds.
- `infra/`: Azure/Terraform deployment helpers plus the eval dashboard (`infra/crs*.service`, `infra/terraform`). These orchestrate the hosted competition environment but are not invoked when running locally (`docker-compose`, `run-crs.sh`, etc.).
- `agent-log-viewer/`: Electron/Next UI that loads `./logs` for humans; it is useful for debugging but not part of the CRS runtime.

## Standalone-network migration plan
1. **Swap out the competition client**: refactor `Submitter` (`crs/app/submitter.py:19-259`) or inject a stub so `submit_pov`, `submit_patch`, and `submit_bundle` keep the existing bookkeeping in `ProductsDB` but no longer call `CompetitionAPIClient`. Add a config flag (`DISABLE_COMPETITION_API` or similar) so the code path is reversible for testing.
2. **Provide a local scoreboard**: reuse `ProductsDB` (already tracking `povs`, `patches`, `bundles`) to feed a lightweight HTTP/CLI UI or just `tail -f logs`. Optionally expose a `/status/` route in the task server that mirrors the old competition counters (`crs/task_server/db.py:32-74`).
3. **Re-create input sources**: replace Azure blobs (`config.CRS_BLOB_ENDPOINT` in `crs/config.py:114-116`) with a self-hosted file server or local disk. Update `azcopy` calls in `crs/modules/fuzzing.py` to point at the new host, or supply a local corpus tarball under `/data` and bypass the download altogether.
4. **Simplify authentication**: drop `API_KEY_ID/TOKEN` requirements inside `crs/task_server/app.py:1-41` by defaulting to open mode for private networks, but keep the hooks so you can optionally require a lightweight secret when tasks arrive from other subnets.
5. **Instrument offline evaluators**: reroute `round_sim.py:1-128` to hit your new task server endpoint, keep the `tests/app/tasks/` library for canned runs, and point `CAPI_URL` at a dummy HTTP listener that simply records bundles instead of rejecting them.

With these steps, you keep the full CRS stack (task server → work queue → agents/fuzzers → submitter hooks) yet avoid DARPA services entirely.
